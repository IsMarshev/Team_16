from __future__ import annotations

# --- –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ ---
import os
import uuid
import requests
from io import BytesIO
from dataclasses import dataclass
from typing import Annotated, Any, Dict, List, Optional, TypedDict
from dotenv import load_dotenv
import numpy as np

# --- –†–∞–±–æ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ –∏ –≥—Ä–∞—Ñ–∏–∫–∞–º–∏ ---
import pandas as pd
from PIL import Image

# --- –ü–∞—Ä—Å–∏–Ω–≥ HTML –∏ Markdown ---
import markdown
from bs4 import BeautifulSoup

# --- LangChain –∏ LangGraph –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã ---
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.language_models import BaseChatModel
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode, tools_condition
from langchain_tavily import TavilySearch

# --- –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∞–≥–µ–Ω—Ç–æ–≤ ---
from src.agents import (
    AgentState,
    RoleAgent,
    SearchAgentState,
    SearchAgent,
    ArticleWriterAgent,
    ReportGenerationState,
    ReportData,
)
from src.config import Config

# --- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è PDF ---
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as ReportLabImage
from reportlab.lib.units import inch
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont

load_dotenv()


class PromptState(TypedDict):
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –≥—Ä–∞—Ñ–∞"""
    messages: List[BaseMessage]

def _merge_dicts(left: dict, right: dict) -> dict:
    return {**left, **right}

class RootState(TypedDict):
    describer_state: Annotated[AgentState, _merge_dicts]
    summarizer_state: Annotated[AgentState, _merge_dicts] 
    prompt_state: Annotated[PromptState, _merge_dicts]
    search_state: Annotated[SearchAgentState, _merge_dicts]
    pdf_state: Annotated[ReportGenerationState, _merge_dicts]


config = Config()

llm = ChatOpenAI(
            api_key=config.api_key.get_secret_value(),
            base_url=config.url,
            model="gemini-2.5-pro-preview",
            temperature=config.websearch.temperature,
            model_kwargs={"max_tokens": 32000},
            extra_body={
                "use_beam_search": config.websearch.use_beam_search, 
                "best_of": config.websearch.best_of
            }
        )

summurize_llm = ChatOpenAI(
            api_key=config.api_key.get_secret_value(),
            base_url=config.url,
            model="qwen3-235b-a22b",
            temperature=config.websearch.temperature,
            model_kwargs={"max_tokens": 32000},
            extra_body={
                "use_beam_search": config.websearch.use_beam_search, 
                "best_of": config.websearch.best_of
            }
        )

article_llm = ChatOpenAI(
            api_key=config.api_key.get_secret_value(),
            base_url=config.url,
            model="gpt-4.1-mini",
            temperature=config.websearch.temperature,
            model_kwargs={"max_tokens": 32000},
            extra_body={
                "use_beam_search": config.websearch.use_beam_search, 
                "best_of": config.websearch.best_of
            }
        )


describer_agent = RoleAgent(
    llm=llm,
    role_name="–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç",
    role_description="You are high skill econometrist and macroeconomist",
    role_instructions=[
        "You are high skill econometrist and macroeconomist. You need describe IRF from large BVAR. It is for economic students and staff of central bank. BVAR model includes real growth rates and inflation of many russian industries plus oil prices growth rate, interest rate and nominal exchange growth rate. Structural shocks are identified with sign restriction plus zero restriction for oil prices. The model include structural break (for all parameters). The list of industries is following: A is  Agriculture, forestry, fishing, and hunting; B is  Mining; C is industrial production; F is Construction; G is  Retail trade, Wholesale trade; H is  Transportation and warehousing; J is  Information; K is  Finance and insurance; L is  Real estate and rental and leasing; M is  Professional and business services, science; O is Government; P is Education; Q is Health care, and social assistance; There are a lot of plots. This plot(includes about 9 subplots with 2 lines on each) that required comments includes IRF for following shock: dpoil. You need describe IRFs and give economic intuition (why such shock should have such influence on the variable). Highlight cross industry effects. Subplot includes response of following variable: dfx. Before break response is following:"
    ],
    additional_context="–¢—ã –≤—Å—Ç—Ä–æ–µ–Ω –≤ –≥—Ä–∞—Ñ –∑–∞–¥–∞—á."
)

summarizer_agent = RoleAgent(
    llm=summurize_llm,
    role_name="–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç",
    role_description="–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–µ—Ä–∞–π—Ç–µ—Ä",
    role_instructions=[
        "–¢–µ–±–µ –Ω–∞–¥–æ —Å—É–º–º–∞—Ä–∏–∑–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –≤ –∫—Ä–∞—Ç–∫—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é",
        "–¢–µ–∫—Å—Ç –Ω–∞–¥–æ —Å–æ–∫—Ä–∞—â–∞—Ç—å —Å —É–º–æ–º, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é",
    ],
    additional_context="–¢—ã –≤—Å—Ç—Ä–æ–µ–Ω –≤ –≥—Ä–∞—Ñ –∑–∞–¥–∞—á."
)

new_agregate_agent = RoleAgent(
    llm=summurize_llm,
    role_name="–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç",
    role_description="–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–µ—Ä–∞–π—Ç–µ—Ä",
    role_instructions=[
        "–¢–µ–±–µ –Ω–∞–¥–æ —Å—É–º–º–∞—Ä–∏–∑–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –≤ –∫—Ä–∞—Ç–∫—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é",
        "–¢–µ–∫—Å—Ç –Ω–∞–¥–æ —Å–æ–∫—Ä–∞—â–∞—Ç—å —Å —É–º–æ–º, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é",
    ],
    additional_context="–¢—ã –≤—Å—Ç—Ä–æ–µ–Ω –≤ –≥—Ä–∞—Ñ –∑–∞–¥–∞—á."
)

prompt_writer = RoleAgent(
    llm=llm,
    role_name="PromptWriter",
    role_description="–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–µ–º–µ",
    role_instructions=[
        "–ü—Ä–æ—á–∏—Ç–∞–π —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –ø—Ä–∏–¥—É–º–∞–π, —á—Ç–æ –∏–º–µ–Ω–Ω–æ —Å—Ç–æ–∏—Ç –∏—Å–∫–∞—Ç—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ.",
        "–í–µ—Ä–Ω–∏ —á–∏—Å—Ç—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤."
    ]
)

search_agent = SearchAgent(config=config)

article_agent = ArticleWriterAgent(llm=article_llm)

class Configuration(TypedDict):
    """Configurable parameters for the agent."""
    # API key for the TavilySearch service
    tavily_api_key: str
    # OpenAI API key for ChatOpenAI
    openai_api_key: str
    # Model name for ChatOpenAI (e.g., "gpt-3.5-turbo")
    model_name: str

def parse_values(s):
    values = []
    for item in s.split(','):
        item = item.strip()
        if item == 'NaN':
            values.append(np.nan)
        else:
            try:
                values.append(float(item))
            except:
                values.append(item)
    return values

def process_excel_data(file_path):

    df = pd.read_excel(file_path, header=None, engine='openpyxl')

    result = {}
    for idx, row in df.iterrows():
        group_name = row[1]
        content = str(row[0])

        if pd.isna(content) or content.strip() == "":
            continue

        shock_blocks = content.split('Subplot includes the following shock:')[1:]
        group_data = []

        for block in shock_blocks:
            shock_line = block.strip().split('\n')[0]
            shock_name = shock_line.split('.')[0].strip()

            if 'Mean is following:' not in block or 'Std is following:' not in block:
                continue

            mean_part = block.split('Mean is following:')[1].split('Std is following:')[0].strip()
            std_part = block.split('Std is following:')[1].split('Subplot includes the following shock:')[0].strip()

            mean_values = parse_values(mean_part)
            std_values = parse_values(std_part)


            shock_data = {
                "data": {
                    "mean": mean_values,
                    "std": std_values
                }
            }
            group_data.append(shock_data)

        result[group_name] = group_data

    return result


def run_load_and_process(root: RootState) -> RootState:
    # 1. –ß–∏—Ç–∞–µ–º Excel (–ø—É—Ç—å –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —á–µ—Ä–µ–∑ root.context –∏–ª–∏ –±—Ä–∞—Ç—å —Å—Ç–∞—Ç–∏—á–Ω–æ)
    df = pd.read_excel(root["describer_state"]["context"]["excel_path"],
                       usecols=["prompt", "fig_name"])
    
    records: list[dict] = []
    for _, row in df.iterrows():
        # 2. –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –≥–æ—Ç–æ–≤–∏–º AgentState
        state: AgentState = {
            "messages": [HumanMessage(content=row["prompt"])],
            "context": {}
        }
        # 3. –ó–∞–ø—É—Å–∫–∞–µ–º describer_agent
        result = describer_agent(state)
        description = result["messages"][-1].content
        graphic_url = result.get("context", {}).get("graphic_url", "")
        
        # 4. –°–æ–±–∏—Ä–∞–µ–º –∑–∞–ø–∏—Å—å
        records.append({
            "id":           row["fig_name"],
            "describe":     description,
            "graphic_url":  f"/Users/ilya/Desktop/work/mlforum/plots/{row["fig_name"]}.png"
        })


    df_out = pd.DataFrame(records)
    out_path = "temp/all_descriptions.parquet"
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    df_out.to_parquet(out_path, index=False, engine="fastparquet")
    print(f"Saved {len(df_out)} records to {out_path}")

    return root



def run_summarize(root: RootState) -> RootState:
    df = pd.read_parquet('temp/all_descriptions.parquet', engine='fastparquet')
    
    summarizes = []
    for item in range(len(df)):
        state: AgentState = {
            "messages": [HumanMessage(content=df['describe'][item])],
            "context": {}
        }
        result = summarizer_agent(state)
        summarizes.append(result["messages"][-1].content)
    df['summarize'] = summarizes
    out_path = 'temp/all_descriptions_and_summary.parquet'
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    df.to_parquet(out_path, index=False, engine="fastparquet")
    print(f"Saved {len(df)} records to {out_path}")
    return root



def write_prompt(root: RootState) -> RootState:
    new_state = prompt_writer(root["prompt_state"])
    root["prompt_state"] = new_state
    # –¥–æ–ø—É—Å—Ç–∏–º, prompt_writer –∫–ª–∞–¥—ë—Ç —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å 
    # –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π message.content, –ø–µ—Ä–µ–Ω–æ—Å–∏–º –µ–≥–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞:
    last = new_state["messages"][-1].content
    # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥ –¥–ª—è SearchAgent
    root["search_state"] = {
        "search_query": last,
        "search_results": [] 
    }
    print(root['search_state'])
    return root

def run_search(root: RootState) -> RootState:
    new_search = search_agent(root["search_state"])
    root["search_state"] = new_search

    search_results = new_search["search_results"]
    records = []
    for result in search_results:
        if isinstance(result, dict):
            records.append({
                "title": result.get("title", ""),
                "texts": result.get("content", ""),
                "url": result.get("url", ""),
                # "date": result.get("date", ""),            
                # "sector": result.get("sector", ""),          
                "relevant score": result.get("relevance_score", 0.0),
            })
        else:
            records.append({
                "title": result.title,
                "texts": result.content,
                "url": result.url,
                # "date": getattr(result, "date", ""),      
                # "sector": getattr(result, "sector", ""),  
                "relevant score": result.relevance_score,
            })

    df = pd.DataFrame(records)

    out_path = "temp/search_results.parquet"
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    df.to_parquet(out_path, index=False, engine="fastparquet")
    print(f"‚úÖ Saved {len(df)} search results to {out_path}")
    
    return root

def run_news_summarize(root: RootState) -> RootState:
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
    df = pd.read_parquet('temp/search_results.parquet', engine='fastparquet')

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –≤ –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π –±–ª–æ–∫ –¥–ª—è –µ–¥–∏–Ω–æ–≥–æ –æ–±–æ–±—â—ë–Ω–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ
    combined_items = []
    for idx, row in df.iterrows():
        combined_items.append({
            "title": row['title'],
            "texts": row['texts'],
            "url": row['url']
        })

    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –¥–ª—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ
    payload = combined_items
    state: AgentState = {
        "messages": [HumanMessage(content=str(payload))],
        "context": {}
    }

    # –ü–æ–ª—É—á–∞–µ–º –µ–¥–∏–Ω—ã–π, –∫—Ä—É–ø–Ω—ã–π –æ–±–æ–±—â—ë–Ω–Ω—ã–π —Ä–µ–∑—é–º–µ –ø–æ –≤—Å–µ–º —Å—Ç–∞—Ç—å—è–º
    result = new_agregate_agent(state)
    big_summary = result["messages"][-1].content

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–º–µ—Å—Ç–µ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    df['summarize'] = big_summary
    out_path = 'temp/search_results_and_aggregate_summary.parquet'
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    df.to_parquet(out_path, index=False, engine="fastparquet")

    print(f"Saved aggregate summary for {len(df)} records to {out_path}")
    return root


def run_article_agent(root: RootState) -> RootState:
    df_graphs = pd.read_parquet("/Users/ilya/Desktop/work/mlforum/Team_16/temp/all_descriptions_and_summary.parquet")
    df_summaries = pd.read_parquet("/Users/ilya/Desktop/work/mlforum/Team_16/temp/search_results_and_aggregate_summary.parquet")

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç—á–µ—Ç–∞
    report: ReportData = {
        'graph_urls': df_graphs['graphic_url'].tolist(),
        'graph_descriptions': df_graphs['describe'].tolist(),
        'annotations': df_graphs['summarize'].tolist(),
        'news_links': df_summaries['url'].tolist(),
        'news_articles': df_summaries['texts'].tolist(),
        'aggregated_news': df_summaries['summarize'].tolist(),
    }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ root
    root['pdf_state']['report_data'] = report
    result = article_agent(root["pdf_state"])
    root["pdf_state"] = result
    return root

def setup_fonts():
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ) –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–µ–º–µ–π—Å—Ç–≤–æ —à—Ä–∏—Ñ—Ç–æ–≤ DejaVu
    –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã –∏ –Ω–∞—á–µ—Ä—Ç–∞–Ω–∏–π (–∂–∏—Ä–Ω—ã–π, –∫—É—Ä—Å–∏–≤).
    """
    print("--- ‚öôÔ∏è  –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —à—Ä–∏—Ñ—Ç–æ–≤ –¥–ª—è PDF ---")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —à—Ä–∏—Ñ—Ç—ã –∏ –∏—Ö URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    font_variants = {
        'DejaVuSans': 'https://github.com/mps/fonts/blob/master/DejaVuSans.ttf?raw=true',
        'DejaVuSans-Bold': 'https://github.com/mps/fonts/blob/master/DejaVuSans-Bold.ttf?raw=true',
        'DejaVuSans-Oblique': 'https://github.com/mps/fonts/blob/master/DejaVuSans-Oblique.ttf?raw=true',
        'DejaVuSans-BoldOblique': 'https://github.com/mps/fonts/blob/master/DejaVuSans-BoldOblique.ttf?raw=true',
    }

    for font_name, url in font_variants.items():
        font_path = f"fonts/{font_name}.ttf"
        if not os.path.exists(font_path):
            print(f"--- üì• –®—Ä–∏—Ñ—Ç {font_path} –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–∫–∞—á–∏–≤–∞—é...")
            try:
                response = requests.get(url)
                response.raise_for_status()
                with open(font_path, 'wb') as f:
                    f.write(response.content)
                print(f"--- ‚úÖ –®—Ä–∏—Ñ—Ç {font_path} —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω.")
            except Exception as e:
                raise IOError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —à—Ä–∏—Ñ—Ç {url}: {e}")
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —à—Ä–∏—Ñ—Ç
        pdfmetrics.registerFont(TTFont(font_name, font_path))

    # –°–≤—è–∑—ã–≤–∞–µ–º —à—Ä–∏—Ñ—Ç—ã –≤ –æ–¥–Ω–æ —Å–µ–º–µ–π—Å—Ç–≤–æ. –≠—Ç–æ –∫–ª—é—á –∫ —Ä–∞–±–æ—Ç–µ <b> –∏ <i>
    pdfmetrics.registerFontFamily(
        'DejaVuSans',
        normal='DejaVuSans',
        bold='DejaVuSans-Bold',
        italic='DejaVuSans-Oblique',
        boldItalic='DejaVuSans-BoldOblique'
    )

def html_to_flowables(html_text, styles):
    """
    –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –∫–æ—Ç–æ—Ä–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∏—Ç–µ—Ä–∏—Ä—É–µ—Ç—Å—è –ø–æ —Ç–µ–≥–∞–º –≤–Ω—É—Ç—Ä–∏ <body>
    –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–ª–æ–∂–µ–Ω–Ω—É—é —Ä–∞–∑–º–µ—Ç–∫—É.
    """
    # 1. ReportLab –ø–æ–Ω–∏–º–∞–µ—Ç <b> –∏ <i>, –ø–æ—ç—Ç–æ–º—É —ç—Ç–∞ –∑–∞–º–µ–Ω–∞ –ø–æ–ª–µ–∑–Ω–∞.
    html_text = (
        html_text
        .replace('<strong>', '<b>').replace('</strong>', '</b>')
        .replace('<em>', '<i>').replace('</em>', '</i>')
    )
    
    soup = BeautifulSoup(html_text, 'lxml')
    flowables = []

    # 2. –ù–∞—Ö–æ–¥–∏–º —Ç–µ–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –ï—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç, —Ä–∞–±–æ—Ç–∞–µ–º —Å —Ç–µ–º —á—Ç–æ –µ—Å—Ç—å.
    body = soup.find('body')
    if not body:
        # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ –ø—Ä–∏—à–µ–ª —Ñ—Ä–∞–≥–º–µ–Ω—Ç HTML –±–µ–∑ <body>, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∞–º soup
        body = soup

    # 3. –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –ü–†–Ø–ú–´–ú –ø–æ—Ç–æ–º–∫–∞–º <body>. –≠—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞.
    for tag in body.find_all(True, recursive=False):
        tag_name = tag.name.lower()
        print(tag_name)
        if tag_name in ['h1', 'h2', 'h3', 'h4']:
            style_name = tag_name.upper() # H1, H2 –∏ —Ç.–¥.
            if style_name in styles:
                flowables.append(Spacer(1, 0.15 * inch))
                flowables.append(Paragraph(tag.decode_contents(), styles[style_name]))
                flowables.append(Spacer(1, 0.1 * inch)) # –ù–µ–±–æ–ª—å—à–æ–π –æ—Ç—Å—Ç—É–ø –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            else:
                 flowables.append(Paragraph(tag.decode_contents(), styles['BodyTextCustom']))

        elif tag_name == 'p':
            flowables.append(Paragraph(tag.decode_contents(), styles['BodyTextCustom']))

        elif tag_name in ['ul', 'ol']:
            # –í–∞—à–∞ –ª–æ–≥–∏–∫–∞ –¥–ª—è —Å–ø–∏—Å–∫–æ–≤ —Ö–æ—Ä–æ—à–∞ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö —Å–ª—É—á–∞–µ–≤
            list_items = []
            for li in tag.find_all('li'):
                # –°–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω Paragraph –¥–ª—è –≤—Å–µ–≥–æ —Å–ø–∏—Å–∫–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º <br/> –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
                list_items.append(f"‚Ä¢ {li.decode_contents()}")
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞ –≤ –æ–¥–∏–Ω –ø–∞—Ä–∞–≥—Ä–∞—Ñ
            if list_items:
                full_list_text = "<br/>".join(list_items)
                flowables.append(Paragraph(full_list_text, styles['ListItem']))
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç –±–µ–∑ —Ç–µ–≥–æ–≤ –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏, –µ–≥–æ –Ω—É–∂–Ω–æ —Ç–æ–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å
        elif tag.name is None and tag.string.strip():
             flowables.append(Paragraph(tag.string, styles['BodyTextCustom']))

        else:
            # –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ç–µ–≥–æ–≤ (div, etc) –ø—Ä–æ—Å—Ç–æ —Ä–µ–Ω–¥–µ—Ä–∏–º –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            flowables.append(Paragraph(tag.decode_contents(), styles['BodyTextCustom']))
            
    return flowables

def generate_pdf_node(root: RootState) -> RootState:
    """
    –£–∑–µ–ª –≥—Ä–∞—Ñ–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–µ—Ç PDF-—Ñ–∞–π–ª –∏–∑ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–∏,
    –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—è –∫–∏—Ä–∏–ª–ª–∏—Ü—É –∏ –ø–æ–ª–Ω—É—é Markdown —Ä–∞–∑–º–µ—Ç–∫—É (–≤–∫–ª—é—á–∞—è –∑–∞–≥–æ–ª–æ–≤–∫–∏).
    """
    # --- 1. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —à—Ä–∏—Ñ—Ç—ã (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è setup_fonts —Å—É—â–µ—Å—Ç–≤—É–µ—Ç) ---
    setup_fonts()
    
    print("--- üìÑ –í—ã–∑–æ–≤ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ PDF —Å –ø–æ–ª–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Markdown ---")

    article_text = root['pdf_state'].get("article_content")
    if not article_text:
        raise ValueError("–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –Ω–µ –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω.")
        
    report_data = root['pdf_state']['report_data']
    output_filename = "analytical_report.pdf"
    
    # --- 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –í–°–ï –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–∏–ª–∏ ---
    FONT_FAMILY_NAME = 'DejaVuSans'
    styles = getSampleStyleSheet()

    # –°—Ç–∏–ª—å –∑–∞–≥–æ–ª–æ–≤–∫–∞ —É—Ä–æ–≤–Ω—è 1 ‚Äî –∂–∏—Ä–Ω—ã–º
    styles.add(ParagraphStyle(
        name='H1',
        parent=styles['h1'],
        fontName='DejaVuSans-Bold',  # –ñ–∏—Ä–Ω—ã–π
        fontSize=20,
        leading=24,
        spaceAfter=16
    ))
    styles.add(ParagraphStyle(
        name='H2',
        parent=styles['h2'],
        fontName='DejaVuSans-Bold',
        fontSize=18,
        leading=22,
        spaceAfter=14
    ))
    styles.add(ParagraphStyle(
        name='H3',
        parent=styles['h3'],
        fontName='DejaVuSans-Bold',
        fontSize=16,
        leading=20,
        spaceAfter=12
    ))
    styles.add(ParagraphStyle(
        name='H4',
        parent=styles['h4'],
        fontName='DejaVuSans-Bold',
        fontSize=14,
        leading=18,
        spaceAfter=10
    ))
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∏–ª–∏
    styles.add(ParagraphStyle(name='ReportTitle', fontName=FONT_FAMILY_NAME, fontSize=24, leading=28, alignment=1, spaceAfter=20))
    styles.add(ParagraphStyle(name='BodyTextCustom', parent=styles['BodyText'], fontName=FONT_FAMILY_NAME, spaceAfter=12))
    
    
    # –°—Ç–∏–ª—å –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å–ø–∏—Å–∫–∞
    styles.add(ParagraphStyle(name='ListItem', parent=styles['BodyText'], fontName=FONT_FAMILY_NAME, leftIndent=20, spaceAfter=6))


    # --- 3. –°–æ–±–∏—Ä–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç (story) ---
    story = []
    story.append(Paragraph("–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç", styles['ReportTitle']))

    text_parts = article_text.split('[GRAPH_')
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
    html_part = markdown.markdown(text_parts[0])
    story.extend(html_to_flowables(html_part, styles))
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏ —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏
    for i, part in enumerate(text_parts[1:]):
        try:
            text_after_graph = part.split(']', 1)[1]
            graph_index = i
            
            if graph_index < len(report_data['graph_urls']):
                url = report_data['graph_urls'][graph_index]
                try:
                    # –í—Å—Ç–∞–≤–ª—è–µ–º –≥—Ä–∞—Ñ–∏–∫
                    print(f"--- üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞: {url} ---")
                    with open(url, "rb") as f:
                        img_data = f.read()
                    pil_image = Image.open(BytesIO(img_data))
                    width, height = letter
                    max_width = width - 2 * inch
                    pil_image.thumbnail((max_width, max_width), Image.Resampling.LANCZOS)
                    story.append(Spacer(1, 0.2 * inch))
                    story.append(ReportLabImage(BytesIO(img_data), width=pil_image.width, height=pil_image.height))
                    story.append(Spacer(1, 0.2 * inch))
                except Exception as e:
                    print(f"--- ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫ {url}: {e} ---")
                    error_html = markdown.markdown(f"**[–û—à–∏–±–∫–∞:** –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ {graph_index + 1}]")
                    story.extend(html_to_flowables(error_html, styles))
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –ü–û–°–õ–ï –≥—Ä–∞—Ñ–∏–∫–∞
            html_after_graph = markdown.markdown(text_after_graph)
            story.extend(html_to_flowables(html_after_graph, styles))

        except IndexError:
            html_part_fallback = markdown.markdown(part)
            story.extend(html_to_flowables(html_part_fallback, styles))

    # --- 4. –°–±–æ—Ä–∫–∞ PDF ---
    doc = SimpleDocTemplate(
        output_filename, pagesize=letter, rightMargin=inch,
        leftMargin=inch, topMargin=inch, bottomMargin=inch
    )
    doc.build(story)
    
    print(f"--- ‚úÖ PDF —Ñ–∞–π–ª —Å –ø–æ–ª–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Markdown —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {output_filename} ---")
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    updated_pdf_state = root['pdf_state'].copy()
    updated_pdf_state["pdf_path"] = output_filename
    new_root = root.copy()
    new_root['pdf_state'] = updated_pdf_state
    return new_root

def barrier_router(state: RootState) -> str:
    return state

def clear_temp(state: RootState) -> str:
    import shutil

    path ="/Users/ilya/Desktop/work/mlforum/Team_16/temp"
    if os.path.exists(path) and os.path.isdir(path):
        try:
            shutil.rmtree(path)
            print(f"–ü–∞–ø–∫–∞ '{path}' —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–∞.")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –ø–∞–ø–∫–∏ '{path}': {e}")
    else:
        print(f"–ü–∞–ø–∫–∞ '{path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")


graph = (
    StateGraph(RootState, config_schema=Configuration)
      .add_node("describer_node", run_load_and_process)
      .add_node("summarize_describer_node", run_summarize)
      .add_node("prompt_node", write_prompt)
      .add_node("search_node", run_search)
      .add_node("news_aggregate", run_news_summarize)
      .add_node("generate_pdf_node", generate_pdf_node)
      .add_node("run_article_agent", run_article_agent)
      .add_node("barrier_node", barrier_router)
      .add_node("clear_temp", clear_temp)

      .add_edge("__start__", "describer_node")
      .add_edge("__start__", "prompt_node")

      .add_edge("describer_node", "summarize_describer_node")
      .add_edge("summarize_describer_node", "barrier_node") 

      .add_edge("prompt_node", "search_node")
      .add_edge("search_node", "news_aggregate")

      .add_edge("news_aggregate", "run_article_agent")
      .add_edge("barrier_node", "run_article_agent")

      .add_edge("run_article_agent", "generate_pdf_node")
      .add_edge("generate_pdf_node",  "clear_temp")
      .add_edge("clear_temp",  END)
      .compile(name="Parallel Two‚ÄëState Graph with Aggregate Summary")
)


